{"cells":[{"cell_type":"markdown","metadata":{"id":"hNgLag1Euy3H"},"source":["# Advanced Programme in Deep Learning (Foundations and Applications)\n","## A Program by IISc and TalentSprint\n","\n","### Mini Project Notebook: To perform text classification of coronavirus tweets during the peak Covid - 19 period using LSTMs/RNNs/CNNs/BERT.\n"]},{"cell_type":"markdown","metadata":{"id":"maritime-miami"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"nljJR6CwfZN_"},"source":["At the end of the mini-hackathon, you will be able to :\n","\n","* perform data preprocessing/preprocess the text\n","* represent the text/words using the pretrained word embeddings - Word2Vec/Glove\n","* build the deep neural network (RNN, LSTM, GRU, CNNs, Bidirectional-LSTM, GRU, BERT) to classify the tweets\n"]},{"cell_type":"markdown","metadata":{"id":"iNI_-0spy1Ho"},"source":["### Introduction\n","\n","First we need to understand why sentiment analysis is needed for social media?\n","\n","People from all around the world have been using social media more than ever. Sentiment analysis on social media data helps to understand the wider public opinion about certain topics such as movies, events, politics, sports, and more and gain valuable insights from this social data. Sentiment analysis has some powerful applications. Nowadays it is also used by some businesses to do market research and understand the customer’s experiences for their products or services.\n","\n","Now an interesting question about this type of problem statement that may arise in your mind is that why sentiment analysis on COVID-19 Tweets? What is about the coronavirus tweets that would be positive? You may have heard sentiment analysis on movie or book reviews, but what is the purpose of exploring and analyzing this type of data?\n","\n","The use of social media for communication during the time of crisis has increased remarkably over the recent years. As mentioned above, analyzing social media data is important as it helps understand public sentiment. During the coronavirus pandemic, many people took to social media to express their anger, grief, or sadness while some also spread happiness and positivity. People also used social media to ask their network for help related to vaccines or hospitals during this hard time. Many issues related to this pandemic can also be solved if experts considered this social data. That’s the reason why analyzing this type of data is important to understand the overall issues faced by people.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FL0Ve1abn6YJ"},"source":["## Dataset\n","\n","The given challenge is to build a multiclass classification model to predict the sentiment of Covid-19 tweets. The tweets have been pulled from Twitter and manual tagging has been done. We are given information like Location, Tweet At, Original Tweet, and Sentiment.\n","\n","The training dataset consists of 36000 tweets and the testing dataset consists of 8955 tweets. There are 5 sentiments namely ‘Positive’, ‘Extremely Positive’, ‘Negative’, ‘Extremely Negative’, and ‘Neutral’ in the sentiment column.\n","\n","## Description\n","\n","This dataset has the following information about the user who tweeted:\n","\n","1. **UserName:** twitter handler\n","2. **ScreenName:** a personal identifier on Twitter and is separate from the username\n","3. **Location:** where in the world the person tweets from\n","4. **TweetAt:** date of the tweet posted (DD-MM-YYYY)\n","5. **OriginalTweet:** the tweet itself\n","6. **Sentiment:** sentiment value\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ih-oasWmdZul"},"source":["## Problem Statement"]},{"cell_type":"markdown","metadata":{"id":"qfWGmjNHdZul"},"source":["To build and implement a multiclass classification deep neural network model to classify between Positive/Extremely Positive/Negative/Extremely Negative/Neutral sentiments"]},{"cell_type":"markdown","metadata":{"id":"1BQEA97zTlTa"},"source":["## Grading"]},{"cell_type":"markdown","metadata":{"id":"DdYmy-tJgURN"},"source":["Here is a handy link to Kaggle's competition documentation (https://www.kaggle.com/docs/competitions), which includes, among other things, instructions on submitting predictions (https://www.kaggle.com/docs/competitions#making-a-submission)."]},{"cell_type":"markdown","metadata":{"id":"Y8OapRtHgLnU"},"source":["## Instructions for downloading train and test dataset from Kaggle API are as follows:"]},{"cell_type":"markdown","metadata":{"id":"DO2jS73oLnCR"},"source":["### 1. Create an API key in Kaggle.\n","\n","To do this, go to the competition site on Kaggle at (https://www.kaggle.com/t/15cef0def403469ebbb5db1a67991873) and open your user settings page. Click Account.\n","\n","* Click on your profile picture at the top-right corner of the page.\n","\n","![alt text](https://i.imgur.com/kSLmEj2.png)\n","\n","* In the popout menu, click the Settings option.\n","\n","![alt text](https://i.imgur.com/tNi6yun.png)\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DkzGffHdbwX2"},"source":["### 2. Next, scroll down to the API access section and click generate to download an API key (kaggle.json).\n","![alt text](https://i.imgur.com/vRNBgrF.png)\n"]},{"cell_type":"markdown","metadata":{"id":"WtETuXx8b-OC"},"source":["### 3. Upload your kaggle.json file using the following snippet in a code cell:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-1pfXBDxWl0Y"},"outputs":[],"source":["from google.colab import files\n","files.upload()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GCV_T6MMW4eX"},"outputs":[],"source":["#If successfully uploaded in the above step, the 'ls' command here should display the kaggle.json file.\n","%ls"]},{"cell_type":"markdown","metadata":{"id":"JbukdzJ6cE32"},"source":["### 4. Install the Kaggle API using the following command\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dMj1n1MJcqzN"},"outputs":[],"source":["!pip install -U -q kaggle==1.5.8"]},{"cell_type":"markdown","metadata":{"id":"3Vpy9P1nchhd"},"source":["### 5. Move the kaggle.json file into ~/.kaggle, which is where the API client expects your token to be located:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yQbPsDOLZ0b4"},"outputs":[],"source":["!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BenAWlpI73sm"},"outputs":[],"source":["# Execute the following command to verify whether the kaggle.json is stored in the appropriate location: ~/.kaggle/kaggle.json\n","!ls ~/.kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vm2jGsCradOS"},"outputs":[],"source":["!chmod 600 /root/.kaggle/kaggle.json # run this command to ensure your Kaggle API token is secure on colab"]},{"cell_type":"markdown","metadata":{"id":"32unPZKzdI72"},"source":["### 6. Now download the Test Data from Kaggle"]},{"cell_type":"markdown","metadata":{"id":"ppuy5gRKHFwv"},"source":["**NOTE: If you get a '404 - Not Found' error after running the cell below, it is most likely that the user (whose kaggle.json is uploaded above) has not 'accepted' the rules of the competition and therefore has 'not joined' the competition.**"]},{"cell_type":"markdown","metadata":{"id":"41-ETZCE_A1j"},"source":["If you encounter **401-unauthorised** download latest **kaggle.json** by repeating steps 1 & 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TY40TmgfHq0s"},"outputs":[],"source":["#If you get a forbidden link, you have most likely not joined the competition.\n","# !kaggle competitions download -c multi-text-classification-of-coronavirus-tweets\n","!kaggle competitions download -c to-classify-coronavirus-tweets-during-covid-19"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mvKRiFNglvpC"},"outputs":[],"source":["!unzip to-classify-coronavirus-tweets-during-covid-19.zip"]},{"cell_type":"markdown","metadata":{"id":"QeKon2vruI_c"},"source":["## YOUR CODING STARTS FROM HERE"]},{"cell_type":"markdown","metadata":{"id":"abstract-stocks"},"source":["## Import required packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O5RcxwQUku6x"},"outputs":[],"source":["# Import required packages\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import nltk\n","import sklearn\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from gensim.utils import simple_preprocess\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","\n","import tensorflow as tf  # use TensorFlow\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.utils import pad_sequences\n","from tensorflow.keras.layers import Input, Embedding, Dense, Bidirectional, Dropout, GRU, LSTM, BatchNormalization, MaxPooling1D,Attention, GlobalAveragePooling1D\n","\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping"]},{"cell_type":"markdown","metadata":{"id":"53g0zVbjRV7K"},"source":["##   **Stage 1**:  Data Loading and Perform Exploratory Data Analysis"]},{"cell_type":"markdown","metadata":{"id":"xIa9LlhMHj5S"},"source":["* Load the Dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6qmR5Vo_tbVy"},"outputs":[],"source":["train = pd.read_csv('corona_nlp_train.csv/corona_nlp_train.csv', encoding='latin1')\n","test = pd.read_csv('corona_nlp_test.csv/corona_nlp_test.csv', encoding='latin1')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-qs6RgDoakwF"},"outputs":[],"source":["train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5siJQ0aQlnH4"},"outputs":[],"source":["test.head()"]},{"cell_type":"markdown","metadata":{"id":"hzQS91rfJLNN"},"source":["* Check for Missing Values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JF3xCD9qJN1c"},"outputs":[],"source":["# missing values in train dataset\n","train.isnull().sum() / len(train) * 100"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4YhnGt6h54GF"},"outputs":[],"source":["# missing values in test dataset\n","test.isnull().sum() / len(train) * 100"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ec513Rt4nDj1"},"outputs":[],"source":["# impute Location missing values with a category 'Not available'\n","train['Location'] = train['Location'].fillna('Not available')\n","test['Location'] = test['Location'].fillna('Not available')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r97Nsh7FqV3M"},"outputs":[],"source":["train['Location'].isnull().sum(), test['Location'].isnull().sum()"]},{"cell_type":"markdown","metadata":{"id":"nra2K6EPHosw"},"source":["* Visualize the sentiment column values\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ksnP-I2Fitd"},"outputs":[],"source":["train['Sentiment'].value_counts() / len(train) * 100"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OMbwzb7GFGb9"},"outputs":[],"source":["sns.countplot(x='Sentiment', data=train)\n","plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n","plt.title('Distribution of Sentiments')\n","plt.xlabel('Sentiment')\n","plt.ylabel('Count')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"_zc6AUq9Hry8"},"source":["* Visualize top 10 Countries that had the highest tweets using countplot (Tweet count vs Location)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M_fUPMJzGl8U"},"outputs":[],"source":["sns.countplot(x='Location', data=train, order=train['Location'].value_counts().iloc[:10].index)\n","plt.xticks(rotation=90)\n","plt.title('Top 10 Countries with Highest Tweet Counts')\n","plt.xlabel('Location')\n","plt.ylabel('Tweet Count')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"GUIM_P-VHuzW"},"source":["* Plotting Pie Chart for the Sentiments in percentage\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s8oRZOYDHAVp"},"outputs":[],"source":["sentiment_counts = train['Sentiment'].value_counts()\n","plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%.2f%%')\n","plt.title('Sentiment Distribution in Tweets')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"cSvzz5z6H8kM"},"source":["* WordCloud for the Tweets/Text\n","\n","    * Visualize the most commonly used words in each sentiment using wordcloud\n","    * Refer to the following [link](https://medium.com/analytics-vidhya/word-cloud-a-text-visualization-tool-fb7348fbf502) for Word Cloud: A Text Visualization tool\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Zn5ZC6WQGRB"},"outputs":[],"source":["from wordcloud import WordCloud, STOPWORDS\n","import matplotlib.pyplot as plt\n","\n","# Concatenate all tweets for each sentiment\n","positive_tweets = \" \".join(train[train['Sentiment'] == 'Positive']['OriginalTweet'])\n","negative_tweets = \" \".join(train[train['Sentiment'] == 'Negative']['OriginalTweet'])\n","neutral_tweets = \" \".join(train[train['Sentiment'] == 'Neutral']['OriginalTweet'])\n","extremely_positive_tweets = \" \".join(train[train['Sentiment'] == 'Extremely Positive']['OriginalTweet'])\n","extremely_negative_tweets = \" \".join(train[train['Sentiment'] == 'Extremely Negative']['OriginalTweet'])\n","\n","\n","# Generate word clouds for each sentiment\n","def generate_wordcloud(input, title):\n","  wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords=STOPWORDS).generate(input)\n","  plt.figure(figsize=(10, 5))\n","  plt.imshow(wordcloud, interpolation='bilinear')\n","  plt.axis('off')\n","  plt.title(title)\n","  plt.show()\n","\n","generate_wordcloud(positive_tweets, 'Wordcloud for Positive Tweets')\n","generate_wordcloud(negative_tweets, 'Wordcloud for Negative Tweets')\n","generate_wordcloud(neutral_tweets, 'Wordcloud for Neutral Tweets')\n","generate_wordcloud(extremely_positive_tweets, 'Wordcloud for Extremely Positive Tweets')\n","generate_wordcloud(extremely_negative_tweets, 'Wordcloud for Extremely Negative Tweets')"]},{"cell_type":"markdown","metadata":{"id":"3oLyIb19KcdL"},"source":["##   **Stage 2**: Data Pre-Processing\n","####  Clean and Transform the data into a specified format\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GTcN5Wel6Y8u"},"outputs":[],"source":["#text preprocessing on the OriginalTweet\n","train['OriginalTweet'] = train['OriginalTweet'].apply(lambda text:simple_preprocess(text, max_len=300))\n","test['OriginalTweet'] = test['OriginalTweet'].apply(lambda text:simple_preprocess(text, max_len=300))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3QaLn2Rv8Kkc"},"outputs":[],"source":["# Remove stop words\n","stop_words = set(stopwords.words('english'))\n","stop_words.remove('not')\n","train['OriginalTweet'] = train['OriginalTweet'].apply(lambda x: [w for w in x if not w in stop_words])\n","test['OriginalTweet'] = test['OriginalTweet'].apply(lambda x: [w for w in x if not w in stop_words])"]},{"cell_type":"markdown","metadata":{"id":"1jZg7yL2TtTM"},"source":["##   **Stage 3**: Build the Word Embeddings using pretrained Word2vec/Glove (Text Representation)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ABQ_Uv6XJN3y"},"outputs":[],"source":["!wget https://nlp.stanford.edu/data/glove.6B.zip\n","!unzip glove.6B.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_9rTpuiwSy0S"},"outputs":[],"source":["embeddings_index = {}\n","# Loading the 300-dimensional vector of the model\n","f = open('glove.6B.300d.txt')\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype=np.float32)\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","print('Found %s word vectors.' % len(embeddings_index))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-7NYMyU0leia"},"outputs":[],"source":["MAX_VOCAB_SIZE = 20000\n","EMBEDDING_DIM = 300\n","MAX_SENT_LEN = 45\n","BATCH_SIZE = 64\n","N_EPOCHS = 30\n","\n","tf.random.set_seed(42)\n","np.random.seed(42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cVCIihAteMi9"},"outputs":[],"source":["tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token='<UNK>')\n","tokenizer.fit_on_texts([' '.join(seq[:MAX_SENT_LEN]) for seq in train['OriginalTweet']])\n","\n","print(\"Number of words in vocabulary:\", len(tokenizer.word_index))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wvDuCyPAdBSm"},"outputs":[],"source":["# Adding 1 because of reversed 0 index\n","words_not_found = []\n","vocab_size = len(tokenizer.word_index) + 1\n","print('Loaded %s word vectors.' % len(embeddings_index))\n","\n","# Create a weight matrix for words in the training data\n","embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n","for word, i in tokenizer.word_index.items():\n","    if i >= vocab_size:\n","      continue\n","    embedding_vector = embeddings_index.get(word)\n","    if (embedding_vector is not None) and len(embedding_vector) > 0:\n","                embedding_matrix[i] = embedding_vector\n","    else:\n","        words_not_found.append(word)"]},{"cell_type":"markdown","metadata":{"id":"I6jfm3YFUL7i"},"source":["##   **Stage 4**: Build and Train the Deep Recurrent Model using Pytorch/Keras\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V8K4OdasNyZk"},"outputs":[],"source":["# Convert the sequence of words to sequnce of indices\n","X = tokenizer.texts_to_sequences([' '.join(seq[:MAX_SENT_LEN]) for seq in train['OriginalTweet']])\n","X = pad_sequences(X, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n","y = train['Sentiment']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5TcgRlWWPD5t"},"outputs":[],"source":["# Converting the labels from categorical to numerical\n","le = LabelEncoder()\n","y = le.fit_transform(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L8f4aAZFwNAf"},"outputs":[],"source":["le.classes_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-5nnz5xzM_eN"},"outputs":[],"source":["# split to train and validation datasets\n","# X_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, random_state=42, train_size=34560)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RMO-l4xvwNAg"},"outputs":[],"source":["# Define the model\n","input_layer = Input(shape=(MAX_SENT_LEN,))\n","embedding_layer = Embedding(vocab_size, EMBEDDING_DIM, weights =[embedding_matrix], trainable=False)(input_layer)\n","bi_lstm1 = Bidirectional(LSTM(128, return_sequences=True, dropout=0.2))(embedding_layer)\n","batch_nor1 = BatchNormalization()(bi_lstm1)\n","bi_lstm2 = Bidirectional(LSTM(128, return_sequences=True, dropout=0.2))(batch_nor1)\n","batch_nor2 = BatchNormalization()(bi_lstm2)\n","bi_lstm3 = Bidirectional(LSTM(64, return_sequences=True, dropout=0.2))(batch_nor2)\n","\n","# Attention mechanism\n","query = Dense(128, kernel_regularizer='l2')(bi_lstm3)  # Use bi_lstm output as query\n","query = Dropout(0.2)(query)  # Dropout applied to the query layer\n","value = Dense(128, kernel_regularizer='l2')(bi_lstm3)  # Use bi_lstm output as value\n","value = Dropout(0.2)(value)  # Dropout applied to the value layer\n","attention_layer = Attention()([query, value])\n","attention_output = GlobalAveragePooling1D()(attention_layer)  # Summarize the attention output\n","\n","# Add Dense Layers after Attention and Pooling\n","dense_1 = Dense(64, activation='relu', kernel_regularizer='l2')(attention_output)\n","dropout1 = Dropout(0.4)\n","\n","# Fully connected output layer\n","output_layer = Dense(5, activation='softmax')(dense_1)\n"]},{"cell_type":"code","source":["# Build and compile the model\n","model = Model(inputs=input_layer, outputs=output_layer)\n","model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"],"metadata":{"id":"6nfPcDxx-Nkh"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5j_puNXFL55Y"},"outputs":[],"source":["print('Summary of the model')\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SxCtpEBSwNAh"},"outputs":[],"source":["early_stopping = EarlyStopping(\n","    monitor='val_accuracy',\n","    patience=8, restore_best_weights=True)"]},{"cell_type":"code","source":["# fit the model\n","history=   model.fit(X, y,\n","                      batch_size=64,\n","                      epochs=30,\n","                      validation_split=0.2)"],"metadata":{"id":"Gl7J2JYp-W6W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k-O0Jx99UhmI"},"source":["##   **Stage 5**: Evaluate the Model and get model predictions on the test dataset\n","\n","* Upload the model predictions to kaggle by mapping the sentiment column vlalues from numericals the categorical\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PSaAlhGGUitF"},"outputs":[],"source":["print('Testing...')\n","model.evaluate(X_val, y_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xLLiGGsjwNAh"},"outputs":[],"source":["def predict_class(text):\n","    '''Function to predict sentiment class of the passed text'''\n","\n","    sentiment_classes = ['Extremely Negative', 'Extremely Positive', 'Negative', 'Neutral',\n","       'Positive']\n","    max_len=MAX_SENT_LEN\n","\n","    # Transforms text to a sequence of integers using a tokenizer object\n","    xt = tokenizer.texts_to_sequences(text)\n","    # Pad sequences to the same length\n","    xt = pad_sequences(xt, padding='post', maxlen=max_len)\n","    # Do the prediction using the loaded model\n","    yt = model.predict(xt).argmax(axis=1)\n","    # Print the predicted sentiment\n","    print('The predicted sentiment is', sentiment_classes[yt[0]])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SeH9iMKLwNAi"},"outputs":[],"source":["X_test = tokenizer.texts_to_sequences([' '.join(seq[:MAX_SENT_LEN]) for seq in test['OriginalTweet']])\n","X_test = pad_sequences(X_test, maxlen=MAX_SENT_LEN, padding='post', truncating='post')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tbdGF92wwNAi"},"outputs":[],"source":["# model predictions on the test data\n","preds = model.predict(X_test)\n","preds.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sK_oWxyqwNAi"},"outputs":[],"source":["# Convert probabilities to class labels (0 through 4 for 5 classes)\n","predicted_classes = np.argmax(preds, axis=1)\n","predicted_classes.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0dUGrXBHwNAi"},"outputs":[],"source":["test['predicted_sentiment'] = predicted_classes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6yFwf68pwNAi"},"outputs":[],"source":["test[\"Sentiment\"] = le.inverse_transform(test['predicted_sentiment'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Di863MSgwNAj"},"outputs":[],"source":["test.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"87xc6I1qwNAj"},"outputs":[],"source":["le.classes_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FhXWRbmOwNAj"},"outputs":[],"source":["test.to_csv('output.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"oKnc1WZk9cIk"},"source":["### Instructions for preparing Kaggle competition predictions\n","\n","\n","* Get the predictions using trained model and prepare a csv file\n","    * DeepNet model gives output for each class, consider the maximum value among all classes as prediction using `np.argmax`.\n","\n","* Predictions (csv) file should contain 2 columns as Sample_Submission.csv\n","  - First column is the Test_Id which is considered as index\n","  - Second column is prediction in decoded form (for eg. Positive, Negative etc...)."]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0rc1"}},"nbformat":4,"nbformat_minor":0}